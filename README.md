# Orion-Dynamic

Orchestrateur de Ressources √† Inventaire Dynamique et Gestion de Baux

## Objectif

Simuler une plateforme PaaS o√π les ressources (conteneurs/machines) **s'enregistrent automatiquement** aupr√®s d'un Control Plane.  
L'orchestrateur g√®re :

- Les locations √† dur√©e d√©termin√©e (baux)
- La QoS avec migration automatique en cas de panne
- Le nettoyage des ressources √† l'expiration des baux

---

## üîπ Architecture : Philosophie Micro-services

Le syst√®me repose sur une s√©paration stricte entre le **Control Plane** (Intelligence) et le **Data Plane** (Ressources).

### 1. Data Plane : Les Workers "Opportunistes" (Push Mode)
Contrairement aux architectures classiques o√π le serveur scanne le r√©seau (Pull), nous utilisons un **mode Push**.
- Chaque Worker embarque un `agent.py` l√©ger.
- Au d√©marrage, l'agent contacte l'API pour signaler sa pr√©sence.
- **Avantage** : Permet de traverser les NAT/Firewalls et d'ajouter des capacit√©s de calcul instantan√©ment sans reconfigurer le serveur central.

### 2. Control Plane : L'intelligence orchestr√©e
Compos√© de micro-services stateless conteneuris√©s :

- **Reverse Proxy (Caddy)** : API Gateway unique. G√®re le **Load Balancing** dynamique vers les r√©plicas d'API.
- **API (FastAPI)** : C≈ìur r√©actif et stateless. G√®re l'enregistrement et les baux.
- **Scheduler** : Assure la coh√©rence (Health Check, Migration, Expiration). Utilise le verrouillage `SKIP LOCKED` pour la scalabilit√©.
- **Autoscaler** : R√©gulation en boucle ferm√©e (PID) qui ajuste les r√©plicas d'API selon la charge CPU.
- **MariaDB** : V√©rit√© terrain. Garantit l'int√©grit√© via des transactions **ACID** strictes (essentiel pour √©viter les doubles locations).
- **Ansible** : Moteur de s√©curit√©. Isole les clients en cr√©ant/supprimant des utilisateurs √©ph√©m√®res sur les workers (garantie de nettoyage sans acc√®s root).

---

## üí° Choix Techniques & R√©silience

### Pourquoi MariaDB & SQL ?
Pour la **Coh√©rence Forte**. Dans un syst√®me de location, deux clients ne doivent jamais obtenir la m√™me ressource. Les transactions `SELECT ... FOR UPDATE` garantissent l'atomicit√© des allocations.

### Gestion de la Concurrence massive
Le Scheduler utilise `SELECT ... FOR UPDATE SKIP LOCKED`.
- Cela permet de lancer plusieurs instances du Scheduler en parall√®le.
- Chaque instance "pioche" une t√¢che libre (ex: migration) sans bloquer les autres.

### S√©curit√© & Isolation
Nous ne donnons jamais d'acc√®s `root` aux clients.
- **Provisioning** : Ansible cr√©e un utilisateur UNIX d√©di√© lors de la location.
- **Nettoyage** : √Ä l'expiration ou apr√®s une panne, Ansible supprime cet utilisateur, garantissant qu'aucune donn√©e r√©siduelle ne persiste pour le client suivant.

### Pivot Technique : Vagrant vs Docker
Initialement pr√©vu sur Vagrant pour une isolation totale, le projet a pivot√© vers une architecture **Docker Native**.
- **Raison** : Instabilit√©s majeures de la virtualisation imbriqu√©e (Linux sur Vagrant sur macOS ARM64/Apple Silicon).
- **B√©n√©fice** : Docker Compose offre ici de meilleures performances et une portabilit√© imm√©diate sur toutes les architectures modernes.

---

## Flowchart

```mermaid
flowchart TD
    %% --- Styles ---
    classDef client fill:#ffecb3,stroke:#ff6f00,stroke-width:2px,color:black;
    classDef control fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:black;
    classDef worker fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px,color:black;
    classDef db fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:black;

    %% --- Noeuds ---
    Client["Client (curl/CLI)"]:::client

    subgraph CONTROL["Control Plane"]
        direction TB
        
        %% Niveau 1 : Entr√©e et Autoscaler align√©s
        RP["Reverse Proxy (Caddy)<br/>:443 Client | :80 Agent"]:::control
        AS["Autoscaler<br/>(Docker Socket)"]:::control
        
        %% Astuce : Lien invisible pour placer AS √† droite de RP
        RP ~~~ AS

        %% Niveau 2 : Services
        API["API Python (Scalable)<br/>:8080"]:::control
        SCHED["Scheduler"]:::control
        
        %% Niveau 3 : Base de donn√©es
        DB[("MariaDB")]:::db
    end

    subgraph DATA["Monde Externe (Data Plane)"]
        Worker["Workers (Pr√©-existants)<br/>Agent 'agent.py'"]:::worker
    end

    %% --- Flux (Edges) ---
    
    %% Flux Entrants
    Client -->|"Flux 1:<br/>POST /api/rent"| RP
    Worker -->|"Flux 0:<br/>POST /api/workers/register"| RP

    %% Flux Internes Control Plane
    %% J'ai mis 3 tirets (--->) ici pour allonger la fl√®che et faire de la place au texte
    RP --->|"Flux 2: Proxy ‚Üí API<br/>(Load Balancing)"| API
    
    API <-->|"Flux 3:<br/>Read/Write"| DB
    SCHED <-->|"Flux 3:<br/>Polling Tasks"| DB
    
    %% Scaling (Lignes pointill√©es pour ne pas surcharger)
    AS -.->|"Monitors CPU<br/>& Scales"| API
    AS -.->|"Monitors CPU<br/>& Scales"| SCHED

    %% Flux Sortants vers Workers
    API -.->|"Flux 4: Provisioning SSH<br/>(Ansible)"| Worker
    SCHED -.->|"Flux 5: Health Check<br/>& Cleanup SSH"| Worker
```# Fonctionnalit√©s

### Agent (`agent.py`)

- Lit les variables d‚Äôenvironnement (`MY_HOST_PORT`, `API_ENDPOINT`)
- Enregistre le Worker via `POST /api/workers/register`
- R√©essaye en boucle si le Control Plane n‚Äôest pas pr√™t
- Envoie `hostname`, `ip`, et `ssh_port`

### API

- **POST /api/workers/register** : enregistre ou met √† jour un Worker
- **POST /api/rent** : loue un ou plusieurs Workers pour une dur√©e d√©finie
  - Retourne les infos SSH pour le client
  - Marque `allocated = true` dans la base de donn√©es

### Scheduler

- **Health Check** : ping SSH tous les Workers
- **Migration** : d√©place les clients d‚Äôun Worker mort vers un Worker sain
- **Expiration des baux** : d√©provisionne et lib√®re automatiquement les Workers

---

## D√©ploiement

1. **Control Plane**

```bash
docker-compose up -d
```
2. **Workers (Data Plane)**

```bash
./launch_workers.sh
```
3. Agent

- Se lance automatiquement au d√©marrage du Worker
- Lit les variables d‚Äôenvironnement (`MY_HOST_PORT`, `API_ENDPOINT`)
- Enregistre le Worker aupr√®s du Control Plane via `POST /api/workers/register`
- G√®re les r√©essais en cas d‚Äô√©chec
- Envoie `hostname`, `ip`, et `ssh_port`

## Fichiers importants

- `docker-compose.yml` : orchestration Control Plane
- `Dockerfile` pour API et Scheduler
- `Dockerfile` pour Workers (Alpine + SSH + Agent)
- `control-plane/autoscaler/` : Code et Dockerfile de l'autoscaler
- `Caddyfile` : configuration du Reverse Proxy
- `init.sql` : initialisation de la base MariaDB
- `playbooks/` : Ansible pour `create_user.yml` et `delete_user.yml`
- `launch_workers.sh` : script pour d√©ployer plusieurs Workers

## üöÄ D√©monstration Compl√®te (`full_demo.sh`)

Le projet inclut un script de d√©monstration complet qui joue un sc√©nario r√©aliste couvrant toutes les fonctionnalit√©s :

1.  **Cleanup** : Nettoie l'environnement.
2.  **Infrastructure** : D√©marre le Control Plane et attend que l'API soit pr√™te.
3.  **Scaling & Load Balancing** :
    *   Scale l'API et le Scheduler √† 3 r√©plicas.
    *   D√©montre le Round-Robin du Load Balancer.
4.  **Authentification & R√¥les** :
    *   Cr√©e un Admin et un User.
    *   V√©rifie les connexions et les tokens JWT.
5.  **Location & Self-Healing** :
    *   L'utilisateur loue un n≈ìud.
    *   **Simulation de panne** : Le worker lou√© est arr√™t√© brutalement.
    *   **Migration** : Le Scheduler d√©tecte la pane et migre l'utilisateur vers un nouveau n≈ìud automatiquement.
    *   **Resurrection** : Le worker mort est red√©marr√©.
    *   **Cleanup de s√©curit√©** : Le Scheduler d√©tecte le retour du worker et supprime imm√©diatement le compte utilisateur qui y √©tait (pour √©viter tout acc√®s non autoris√©).
6.  **Cycle de vie** :
    *   Location -> Extension de bail -> Lib√©ration anticip√©e.
    *   V√©rification que les ressources sont bien lib√©r√©es.

### Lancer la d√©mo

```bash
cd orion-dynamic
./full_demo.sh
```

> **Note** : Le script est interactif et vous guidera √©tape par √©tape.

## fichiers importants

- `full_demo.sh` : Script de d√©monstration principal.
- `start_demo.sh` : Script utilitaire pour lancer l'infrastructure.
- `docker-compose.yml` : Orchestration Control Plane.
- `Dockerfile` pour API et Scheduler.
- `control-plane/autoscaler/` : Code de l'autoscaler.
- `Caddyfile` : Configuration du Reverse Proxy.

## ‚úÖ Tests Unitaires

Une suite de tests compl√®te (**58 tests**, couverture ~82%) couvre l'ensemble des composants critiques :
- **Autoscaler (90%)** : Scaling, Commandes Docker, Gestion d'erreurs.
- **API (86%)** : Endpoints, Authentification, cas limites et erreurs DB.
- **Scheduler (76%)** : Boucle principale robuste, Health Check, Migration, Expiration.

Les tests utilisent `pytest` avec un mock complet de la base de donn√©es et d'Ansible, permettant une ex√©cution rapide et isol√©e.

### Lancer les tests

```bash
cd orion-dynamic
./run_unit_tests.sh
```
Ce script configure automatiquement un environnement virtuel (`venv_test`), installe les d√©pendances et lance les tests avec un rapport de couverture.

## API Endpoints et Commandes

### Authentification

- **POST /api/signup**
  - Cr√©e un utilisateur.
  - Body JSON : 
    ```json
    {"username":"user", "password":"pass"}
    ```
  - Retour : 
    ```json
    {"message":"Compte cr√©√©"}
    ```
    ou erreur.

- **POST /api/login**
  - Connecte un utilisateur et retourne un JWT.
  - Body JSON : 
    ```json
    {"username":"user", "password":"pass"}
    ```
  - Retour : 
    ```json
    {"token": "<JWT>"}
    ```

### Gestion des Workers / Location

- **POST /api/rent**
  - Loue un ou plusieurs Workers.
  - Headers : `Authorization: Bearer <token>`
  - Body JSON : 
    ```json
    {"duration_hours": 2, "count": 1, "ssh_password": "optionnel"}
    ```
  - Retour : liste des locations :
    ```json
    [
      {
        "rental_id": 2,
        "host_ip": "192.168.0.10",
        "ssh_port": 22221,
        "client_user": "alice",
        "client_pass": "motdepasse123",
        "leased_until": "2025-11-15T21:46:30.988892"
      }
    ]
    ```

- **POST /api/release/<rental_id>**
  - Lib√®re un bail existant.
  - Headers : `Authorization: Bearer <token>`
  - Retour : 
    ```json
    {"message":"Lease released"}
    ```

- **POST /api/extend/<rental_id>**
  - Prolonge un bail.
  - Headers : `Authorization: Bearer <token>`
  - Body JSON : 
    ```json
    {"additional_hours": 1}
    ```
  - Retour : 
    ```json
    {"lease_id": 2, "new_end_at": "2025-11-15T22:46:30.988892"}
    ```

- **GET /api/nodes**
  - Liste les Workers et leurs locations.
  - Headers : `Authorization: Bearer <token>`
  - Retour : liste des n≈ìuds avec infos de bail si actif :
    ```json
    [
      {
        "node_id": 1,
        "hostname": "worker1",
        "ssh_port": 22221,
        "status": "alive",
        "allocated": true,
        "lease": {
          "rental_id": 2,
          "user_id": 1,
          "leased_from": "2025-11-15T20:46:30.988892",
          "leased_until": "2025-11-15T21:46:30.988892",
          "active": true
        }
      }
    ]
    ```

- **POST /api/workers/register**
  - Appel√© par l‚Äôagent des Workers.
  - Body JSON : 
    ```json
    {"hostname":"host", "ip":"1.2.3.4", "ssh_port":2222}
    ```
  - Retour : 
    ```json
    {"message": "Worker enregistr√© avec succ√®s"}
    ```
    ou 
    ```json
    {"message": "Worker d√©j√† enregistr√©"}
    ```

- **GET /api/health**
  - V√©rifie l‚Äô√©tat du serveur.
  - Retour : 
    ```json
    {"status": "healthy"}
    ```



## Notes

- Pour les Workers Docker, `host.docker.internal` est utilis√© √† la place de l‚ÄôIP interne Docker pour les connexions SSH depuis le Control Plane.
- La base MariaDB stocke :
  - `nodes` : √©tat des Workers
  - `users` : utilisateurs
  - `rentals` : baux actifs

